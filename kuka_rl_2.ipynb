{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kuka\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### Start the Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you're in the right virtual environment and the right python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python --version\n",
    "# !pip install pybullet\n",
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from collections import deque\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from pybullet_envs.bullet.kuka_diverse_object_gym_env import KukaDiverseObjectEnv\n",
    "from gym import spaces\n",
    "import pybullet as p\n",
    "\n",
    "env = KukaDiverseObjectEnv(renders=False, isDiscrete=False, removeHeightHack=False, maxSteps=20)\n",
    "env.cid = p.connect(p.DIRECT)\n",
    "action_space = spaces.Box(low=-1, high=1, shape=(5,1))\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-Critic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_hidden_layer(input_dim, hidden_layers):\n",
    "    \"\"\"Build hidden layer.\n",
    "    Params\n",
    "    ======\n",
    "        input_dim (int): Dimension of hidden layer input\n",
    "        hidden_layers (list(int)): Dimension of hidden layers\n",
    "    \"\"\"\n",
    "    hidden = nn.ModuleList([nn.Linear(input_dim, hidden_layers[0])])\n",
    "    if len(hidden_layers)>1:\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        hidden.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "    return hidden\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,state_size,action_size,shared_layers,\n",
    "                 critic_hidden_layers=[],actor_hidden_layers=[],\n",
    "                 seed=0, init_type=None):\n",
    "        \"\"\"Initialize parameters and build policy.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int,int,int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            shared_layers (list(int)): Dimension of the shared hidden layers\n",
    "            critic_hidden_layers (list(int)): Dimension of the critic's hidden layers\n",
    "            actor_hidden_layers (list(int)): Dimension of the actor's hidden layers\n",
    "            seed (int): Random seed\n",
    "            init_type (str): Initialization type\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.init_type = init_type\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.sigma = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "        # Add shared hidden layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(state_size[0])))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(state_size[1])))\n",
    "        linear_input_size = convh * convw * 32\n",
    "        self.shared_layers = build_hidden_layer(input_dim=linear_input_size,\n",
    "                                                hidden_layers=shared_layers)\n",
    "\n",
    "        # Add critic layers\n",
    "        if critic_hidden_layers:\n",
    "            # Add hidden layers for critic net if critic_hidden_layers is not empty\n",
    "            self.critic_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                    hidden_layers=critic_hidden_layers)\n",
    "            self.critic = nn.Linear(critic_hidden_layers[-1], 1)\n",
    "        else:\n",
    "            self.critic_hidden = None\n",
    "            self.critic = nn.Linear(shared_layers[-1], 1)\n",
    "\n",
    "        # Add actor layers\n",
    "        if actor_hidden_layers:\n",
    "            # Add hidden layers for actor net if actor_hidden_layers is not empty\n",
    "            self.actor_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                   hidden_layers=actor_hidden_layers)\n",
    "            self.actor = nn.Linear(actor_hidden_layers[-1], action_size)\n",
    "        else:\n",
    "            self.actor_hidden = None\n",
    "            self.actor = nn.Linear(shared_layers[-1], action_size)\n",
    "\n",
    "        # Apply Tanh() to bound the actions\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Initialize hidden and actor-critic layers\n",
    "        if self.init_type is not None:\n",
    "            self.shared_layers.apply(self._initialize)\n",
    "            self.critic.apply(self._initialize)\n",
    "            self.actor.apply(self._initialize)\n",
    "            if self.critic_hidden is not None:\n",
    "                self.critic_hidden.apply(self._initialize)\n",
    "            if self.actor_hidden is not None:\n",
    "                self.actor_hidden.apply(self._initialize)\n",
    "\n",
    "    def _initialize(self, n):\n",
    "        \"\"\"Initialize network weights.\n",
    "        \"\"\"\n",
    "        if isinstance(n, nn.Linear):\n",
    "            if self.init_type=='xavier-uniform':\n",
    "                nn.init.xavier_uniform_(n.weight.data)\n",
    "            elif self.init_type=='xavier-normal':\n",
    "                nn.init.xavier_normal_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-uniform':\n",
    "                nn.init.kaiming_uniform_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-normal':\n",
    "                nn.init.kaiming_normal_(n.weight.data)\n",
    "            elif self.init_type=='orthogonal':\n",
    "                nn.init.orthogonal_(n.weight.data)\n",
    "            elif self.init_type=='uniform':\n",
    "                nn.init.uniform_(n.weight.data)\n",
    "            elif self.init_type=='normal':\n",
    "                nn.init.normal_(n.weight.data)\n",
    "            else:\n",
    "                raise KeyError('initialization type is not found in the set of existing types')\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> (action, value).\"\"\"\n",
    "        def apply_multi_layer(layers,x,f=F.leaky_relu):\n",
    "            for layer in layers:\n",
    "                x = f(layer(x))\n",
    "            return x\n",
    "\n",
    "        state = F.relu(self.bn1(self.conv1(state)))\n",
    "        state = F.relu(self.bn2(self.conv2(state)))\n",
    "        state = F.relu(self.bn3(self.conv3(state)))\n",
    "        state = apply_multi_layer(self.shared_layers,state.view(state.size(0),-1))\n",
    "\n",
    "        v_hid = state\n",
    "        if self.critic_hidden is not None:\n",
    "            v_hid = apply_multi_layer(self.critic_hidden,v_hid)\n",
    "\n",
    "        a_hid = state\n",
    "        if self.actor_hidden is not None:\n",
    "            a_hid = apply_multi_layer(self.actor_hidden,a_hid)\n",
    "\n",
    "        a = self.tanh(self.actor(a_hid))\n",
    "        value = self.critic(v_hid).squeeze(-1)\n",
    "        return a, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    #env.render(mode='human')\n",
    "    screen = env._get_observation().transpose((2, 0, 1))\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# number of agents\n",
    "num_agents = 1\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(init_screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "i_episode = 0\n",
    "ten_rewards = 0\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "\n",
    "    global i_episode \n",
    "    global ten_rewards\n",
    "    global writer\n",
    "    \n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "    value_list=[]\n",
    "    done_list=[]\n",
    "\n",
    "    state = envs.reset()\n",
    "\n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        action = np.random.randn(action_size)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        _, reward, done, _  = envs.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "\n",
    "    for t in range(tmax):\n",
    "        states = get_screen()\n",
    "        action_est, values = policy(states)\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        log_probs = torch.sum(log_probs, dim=-1).detach()\n",
    "        values = values.detach()\n",
    "        actions = actions.detach()\n",
    "        \n",
    "        env_actions = actions.cpu().numpy()\n",
    "        _, reward, done, _  = envs.step(env_actions[0])\n",
    "        rewards = torch.tensor([reward], device=device)\n",
    "        dones = torch.tensor([done], device=device)\n",
    "\n",
    "        state_list.append(states.unsqueeze(0))\n",
    "        prob_list.append(log_probs.unsqueeze(0))\n",
    "        action_list.append(actions.unsqueeze(0))\n",
    "        reward_list.append(rewards.unsqueeze(0))\n",
    "        value_list.append(values.unsqueeze(0))\n",
    "        done_list.append(dones)\n",
    "\n",
    "        if np.any(dones.cpu().numpy()):\n",
    "            ten_rewards += reward\n",
    "            i_episode += 1\n",
    "            state = envs.reset()\n",
    "            if i_episode%10 == 0:\n",
    "                writer.add_scalar('ten episodes average rewards', ten_rewards/10.0, i_episode)\n",
    "                ten_rewards = 0\n",
    "\n",
    "    state_list = torch.cat(state_list, dim=0)\n",
    "    prob_list = torch.cat(prob_list, dim=0)\n",
    "    action_list = torch.cat(action_list, dim=0)\n",
    "    reward_list = torch.cat(reward_list, dim=0)\n",
    "    value_list = torch.cat(value_list, dim=0)\n",
    "    done_list = torch.cat(done_list, dim=0)\n",
    "    return prob_list, state_list, action_list, reward_list, value_list, done_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(rewards, values, dones):\n",
    "    n_step = len(rewards)\n",
    "    n_agent = len(rewards[0])\n",
    "\n",
    "    # Create empty buffer\n",
    "    GAE = torch.zeros(n_step,n_agent).float().to(device)\n",
    "    returns = torch.zeros(n_step,n_agent).float().to(device)\n",
    "\n",
    "    # Set start values\n",
    "    GAE_current = torch.zeros(n_agent).float().to(device)\n",
    "\n",
    "    TAU = 0.99\n",
    "    discount = 0.98\n",
    "    values_next = values[-1].detach()\n",
    "    returns_current = values[-1].detach()\n",
    "    for irow in reversed(range(n_step)):\n",
    "        values_current = values[irow]\n",
    "        rewards_current = rewards[irow]\n",
    "        gamma = discount * (1. - dones[irow].float())\n",
    "\n",
    "        # Calculate TD Error\n",
    "        td_error = rewards_current + gamma * values_next - values_current\n",
    "        # Update GAE, returns\n",
    "        GAE_current = td_error + gamma * TAU * GAE_current\n",
    "        returns_current = rewards_current + gamma * returns_current\n",
    "        # Set GAE, returns to buffer\n",
    "        GAE[irow] = GAE_current\n",
    "        returns[irow] = returns_current\n",
    "\n",
    "        values_next = values_current\n",
    "\n",
    "    return GAE, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(envs, policy, tmax=1000):\n",
    "    reward_list=[]\n",
    "    state = envs.reset()\n",
    "    for t in range(tmax):\n",
    "        states = get_screen()\n",
    "        action_est, values = policy(states)\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        _, reward, done, _  = envs.step(actions[0])\n",
    "        dones = done\n",
    "        reward_list.append(np.mean(reward))\n",
    "\n",
    "        # stop if any of the trajectories is done to have retangular lists\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "An actor-critic structure with continuous action space is used for this project. The policy consists of 3 parts, a shared hidden layers, actor, and critic.\n",
    "The actor layer outputs the mean value of a normal distribution, from which the agent's action is sampled. The critic layer yields the value function.\n",
    "\n",
    "- Shared layer:\n",
    "```\n",
    "Input State(48,48,3) -> Conv2d(3, 16, 5, 2) -> BatchNorm2d(16) -> Conv2d(16, 32, 5, 2)-> BatchNorm2d(32)\n",
    "-> Conv2d(32, 32, 5, 2) -> BatchNorm2d(32) -> Dense(128) -> LeakyReLU -> Dense(128) -> LeakyReLU -> Dense(64) -> LeakyReLU\n",
    "```\n",
    "- Actor and Critic layers:\n",
    "```\n",
    "LeakyRelu -> Dense(64) -> LeakyRelu -> Dense(4)-> tanh -> Actor's output\n",
    "LeakyReLU -> Dense(64) -> LeakyRelu -> Dense(1) -> Critic's output\n",
    "```\n",
    "\n",
    "### Model update using PPO/GAE\n",
    "The hyperparameters used during training are:\n",
    "\n",
    "Parameter | Value | Description\n",
    "------------ | ------------- | -------------\n",
    "Number of Agents | 1 | Number of agents trained simultaneously\n",
    "tmax | 20 | Maximum number of steps per episode\n",
    "Epochs | 10 | Number of training epoch per batch sampling\n",
    "Batch size | 128 | Size of batch taken from the accumulated  trajectories\n",
    "Discount (gamma) | 0.993 | Discount rate \n",
    "Epsilon | 0.07 | Ratio used to clip r = new_probs/old_probs during training\n",
    "Gradient clip | 10.0 | Maximum gradient norm \n",
    "Beta | 0.01 | Entropy coefficient \n",
    "Tau | 0.95 | tau coefficient in GAE\n",
    "Learning rate | 2e-4 | Learning rate \n",
    "Optimizer | Adam | Optimization method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your own policy!\n",
    "policy=ActorCritic(state_size=(screen_height, screen_width),\n",
    "              action_size=action_size,\n",
    "              shared_layers=[128, 64],\n",
    "              critic_hidden_layers=[64],\n",
    "              actor_hidden_layers=[64],\n",
    "              init_type='xavier-uniform',\n",
    "              seed=0).to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "optimizer = optim.Adam(policy.parameters(), lr=6e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'policy_ppo.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "best_mean_reward = None\n",
    "\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "discount = 0.993\n",
    "epsilon = 0.07\n",
    "beta = .01\n",
    "opt_epoch = 10\n",
    "season = 1000000\n",
    "batch_size = 128\n",
    "tmax = 1000 #env episode steps\n",
    "save_scores = []\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for s in range(season):\n",
    "    policy.eval()\n",
    "    old_probs_lst, states_lst, actions_lst, rewards_lst, values_lst, dones_list = collect_trajectories(envs=env,\n",
    "                                                                                                       policy=policy,\n",
    "                                                                                                       tmax=tmax,\n",
    "                                                                                                       nrand = 5)\n",
    "\n",
    "    season_score = rewards_lst.sum(dim=0).item()\n",
    "    scores_window.append(season_score)\n",
    "    save_scores.append(season_score)\n",
    "    \n",
    "    gea, target_value = calc_returns(rewards = rewards_lst,\n",
    "                                     values = values_lst,\n",
    "                                     dones=dones_list)\n",
    "    gea = (gea - gea.mean()) / (gea.std() + 1e-8)\n",
    "\n",
    "    policy.train()\n",
    "\n",
    "    # cat all agents\n",
    "    def concat_all(v):\n",
    "        #print(v.shape)\n",
    "        if len(v.shape) == 3:#actions\n",
    "            return v.reshape([-1, v.shape[-1]])\n",
    "        if len(v.shape) == 5:#states\n",
    "            v = v.reshape([-1, v.shape[-3], v.shape[-2],v.shape[-1]])\n",
    "            #print(v.shape)\n",
    "            return v\n",
    "        return v.reshape([-1])\n",
    "\n",
    "    old_probs_lst = concat_all(old_probs_lst)\n",
    "    states_lst = concat_all(states_lst)\n",
    "    actions_lst = concat_all(actions_lst)\n",
    "    rewards_lst = concat_all(rewards_lst)\n",
    "    values_lst = concat_all(values_lst)\n",
    "    gea = concat_all(gea)\n",
    "    target_value = concat_all(target_value)\n",
    "\n",
    "    # gradient ascent step\n",
    "    n_sample = len(old_probs_lst)//batch_size\n",
    "    idx = np.arange(len(old_probs_lst))\n",
    "    np.random.shuffle(idx)\n",
    "    for epoch in range(opt_epoch):\n",
    "        for b in range(n_sample):\n",
    "            ind = idx[b*batch_size:(b+1)*batch_size]\n",
    "            g = gea[ind]\n",
    "            tv = target_value[ind]\n",
    "            actions = actions_lst[ind]\n",
    "            old_probs = old_probs_lst[ind]\n",
    "\n",
    "            action_est, values = policy(states_lst[ind])\n",
    "            sigma = nn.Parameter(torch.zeros(action_size))\n",
    "            dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            log_probs = torch.sum(log_probs, dim=-1)\n",
    "            entropy = torch.sum(dist.entropy(), dim=-1)\n",
    "\n",
    "            ratio = torch.exp(log_probs - old_probs)\n",
    "            ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            L_CLIP = torch.mean(torch.min(ratio*g, ratio_clipped*g))\n",
    "            # entropy bonus\n",
    "            S = entropy.mean()\n",
    "            # squared-error value function loss\n",
    "            L_VF = 0.5 * (tv - values).pow(2).mean()\n",
    "            # clipped surrogate\n",
    "            L = -(L_CLIP - L_VF + beta*S)\n",
    "            optimizer.zero_grad()\n",
    "            # This may need retain_graph=True on the backward pass\n",
    "            # as pytorch automatically frees the computational graph after\n",
    "            # the backward pass to save memory\n",
    "            # Without this, the chain of derivative may get lost\n",
    "            L.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "            del(L)\n",
    "\n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.998\n",
    "\n",
    "    mean_reward = np.mean(scores_window)\n",
    "    writer.add_scalar(\"epsilon\", epsilon, s)\n",
    "    writer.add_scalar(\"beta\", beta, s)\n",
    "    # display some progress every n iterations\n",
    "    if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                # For saving the model and possibly resuming training\n",
    "                torch.save({\n",
    "                        'policy_state_dict': policy.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epsilon': epsilon,\n",
    "                        'beta': beta\n",
    "                        }, PATH)\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "                best_mean_reward = mean_reward\n",
    "    if s>=25 and mean_reward>35:\n",
    "        print('Environment solved in {:d} seasons!\\tAverage Score: {:.2f}'.format(s+1, mean_reward))\n",
    "        break\n",
    "\n",
    "\n",
    "print('Average Score: {:.2f}'.format(mean_reward))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time: {}\".format(timedelta(seconds=elapsed)))\n",
    "writer.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(save_scores)), save_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Season #')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, reward: 0.0\n",
      "Episode: 2, reward: 0.0\n",
      "Green block has been detected\n",
      "Episode: 3, reward: 1.0\n",
      "Green block has been detected\n",
      "Episode: 4, reward: 1.0\n",
      "Green block has been detected\n",
      "Episode: 5, reward: 1.0\n",
      "Green block has been detected\n",
      "Episode: 6, reward: 1.0\n",
      "Green block has been detected\n",
      "Episode: 7, reward: 1.0\n",
      "Episode: 8, reward: 0.0\n",
      "Episode: 9, reward: 0.0\n",
      "Green block has been detected\n",
      "Episode: 10, reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "episode = 10\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "env = KukaDiverseObjectEnv(renders=False, isDiscrete=False, removeHeightHack=False, maxSteps=20, isTest=True)\n",
    "env.cid = p.connect(p.DIRECT)\n",
    "# load the model\n",
    "checkpoint = torch.load(PATH)\n",
    "policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "\n",
    "# evaluate the model\n",
    "for e in range(episode):\n",
    "    rewards = eval_policy(envs=env, policy=policy)\n",
    "    reward = np.sum(rewards,0)\n",
    "    print(\"Episode: {0:d}, reward: {1}\".format(e+1, reward), end=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
